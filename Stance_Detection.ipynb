{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Stance for Fake News Identification\n",
    "\n",
    "We try to tackle the issue of fake news using Artificial Intelligence and Machine Learning. According to [fakenewschallenge.org](fakenewschallenge.org), first step towards identifying fake news is to see what other news sources are talking about the same news.\n",
    "\n",
    "We use the dataset provided by fakenewschallenge.org. The task is to identify whether the given body is 'related' or 'unrelated' to the title. If related, does it 'discuss', 'agree', 'disagree', with the given title.\n",
    "\n",
    "I have referred the approach of Benjamin Riedel et al. mentioned in their [paper](https://arxiv.org/abs/1707.03264) - \"A simple but tough-to-beat baseline for the Fake News Challenge stance detection task\". The approach uses a sinlge layer neural network as a classifier. The features are identified as concatination of TF-vector of Headline, TF-vector of Body, and TF-IDF cosine similarity between the headline and the body. \n",
    "\n",
    "The mentioned apporach has been optimized for getting a good score on fakenewschallenge.org's scoring system. As the scoring system gives more weightage to the distinction between 'Unrelated' and 'Related', the accuracy of the mentioned approach is high for 'Unrelated' classes. It is very low for 'disagree' (6.6%) labelled data. I have tried to improve the algorithm to perform equally on all classes. \n",
    "\n",
    "I have used undersampling and oversampling techniques to balance the training data. Further, I have given class weights to specify to the model that classification of all classes is equally important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import optimizers\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.utils import class_weight\n",
    "import nltk\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare The Training data:\n",
    "\n",
    "The training data consists of 49972 instances the distribution of the label is as follows:\n",
    "\n",
    "```\n",
    "unrelated \tdiscuss \tagree \t\tdisagree\n",
    "73.13%      17.82% \t    7.36% \t    1.68%\n",
    "```\n",
    "\n",
    "This is highly imbalanced data. Hence we will try to balance it.\n",
    "\n",
    "We will pick 3000 instances from each of 'unrelated','agree','disagree' data. \n",
    "We have additional data given by fakenewschallenge.org for testing. We will pick 347(50%) instances from 'disagree' label, and add it to the train data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balnced Train Data:\n",
      "           Headline  Body ID\n",
      "Stance                      \n",
      "agree          3000     3000\n",
      "disagree       1188     1188\n",
      "discuss        3000     3000\n",
      "unrelated      3000     3000\n"
     ]
    }
   ],
   "source": [
    "balance_data = pd.DataFrame()\n",
    "\n",
    "original_data = pd.read_csv( 'train_stances.csv' )\n",
    "original_test_data = pd.read_csv( 'competition_test_stances.csv' )\n",
    "\n",
    "disagree_data = original_data.loc[original_data['Stance']=='disagree']\n",
    "disagree_data_test = original_test_data.loc[original_test_data['Stance']=='disagree']\n",
    "disagree_data_test=disagree_data_test.head(348)\n",
    "disagree_data=disagree_data.append(disagree_data_test)\n",
    "\n",
    "balance_data = balance_data.append(disagree_data)\n",
    "\n",
    "stances = original_test_data['Stance'].unique().tolist()\n",
    "\n",
    "for stance in stances:\n",
    "    if stance != 'disagree':\n",
    "        stance_data = original_data.loc[original_data['Stance']==stance]\n",
    "        stance_data=stance_data.sample(3000)\n",
    "        balance_data=balance_data.append(stance_data,ignore_index=True)\n",
    "\n",
    "\n",
    "balance_data= balance_data.sample(frac=1)\n",
    "print('Balnced Train Data:')\n",
    "print(balance_data.groupby('Stance').count())\n",
    "\n",
    "balance_data.to_csv('my_train_stances.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFiles(bodies,stances):\n",
    "    bodiesFile=pd.read_csv(bodies)\n",
    "    stancesFile = pd.read_csv(stances)\n",
    "    consolidated_data=pd.merge(bodiesFile,stancesFile, on=['Body ID'])\n",
    "    consolidated_data['merged'] = consolidated_data['Headline']+' '+consolidated_data['articleBody']\n",
    "    return {\n",
    "        'consolidated_data':consolidated_data,\n",
    "        'bodiesFile':bodiesFile,\n",
    "        'stancesFile':stancesFile\n",
    "    }\n",
    "\n",
    "def getCorpus(data):\n",
    "    unique_headlines = data['Headline'].unique().tolist()\n",
    "    unique_bodies = data['articleBody'].unique().tolist()\n",
    "    corpus = unique_headlines + unique_bodies\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the labels in Training Data:\n",
      "unrelated - 2384\n",
      "agree - 2422\n",
      "discuss - 2389\n",
      "disagree - 677\n",
      "\n",
      "\n",
      "Distribution of the labels in Test Data:\n",
      "unrelated 616\n",
      "agree 578\n",
      "discuss 611\n",
      "disagree 163\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "merged_data=readFiles('train_bodies.csv','my_train_stances.csv')['consolidated_data']\n",
    "train_data, test_data = train_test_split(merged_data, test_size=0.2)\n",
    "\n",
    "print('Distribution of the labels in Training Data:')\n",
    "train_labels = train_data.groupby('Stance').size()\n",
    "for stance in stances:\n",
    "    print(str(stance), '-',int(train_labels[stance]))\n",
    "\n",
    "print('\\n\\nDistribution of the labels in Test Data:')\n",
    "test_labels = test_data.groupby('Stance').size()\n",
    "for stance in stances:\n",
    "    print(stance, test_labels[stance])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus=getCorpus(train_data)\n",
    "test_corpus=getCorpus(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "top_N = 3000\n",
    "\n",
    "\n",
    "def preprocess(data_frame,corpus, countVectorer,train=False):      \n",
    "        \n",
    "    corpus = [' '.join([stemmer.stem(word) for word in str.split(' ')]) for str in corpus]\n",
    "    if (train == True):\n",
    "        cvectorizer = CountVectorizer( lowercase=True,\n",
    "                                       stop_words=text.ENGLISH_STOP_WORDS, token_pattern=\"\\w+[\\-\\']?\\w+\", max_features=top_N, ngram_range=(1,4) )\n",
    "        countVectorer = cvectorizer.fit( corpus );\n",
    "\n",
    "    cvec = countVectorer.transform( corpus )\n",
    "  #  print(countVectorer.vocabulary_)\n",
    "\n",
    "    idxToContentMap = {}\n",
    "    for index, element in enumerate( corpus ):\n",
    "        idxToContentMap[element] = index\n",
    "\n",
    "    tfreq_transformer = TfidfTransformer( use_idf=False ).fit( cvec )  # transform the counts to normalized tf.\n",
    "    tfreq = tfreq_transformer.transform( cvec ).toarray()\n",
    "\n",
    "    tfidf_vector = TfidfVectorizer( max_features=top_N, lowercase=True,\n",
    "                                    stop_words=text.ENGLISH_STOP_WORDS, token_pattern=\"\\w+[\\-\\']?\\w+\",ngram_range=(1,4) ) \\\n",
    "        .fit_transform( corpus )  # Train and test sets\n",
    "    features = []\n",
    "    feat = np.array( [] )\n",
    "    cosineMap = {}\n",
    "    for index, row in data_frame.iterrows():\n",
    "        body_id = row['Body ID']\n",
    "        Headline = ' '.join(stemmer.stem(word) for word in row['Headline'].split(\" \"))\n",
    "        body = ' '.join(stemmer.stem(word) for word in row['articleBody'].split(\" \"))\n",
    "        head_index = idxToContentMap[Headline]\n",
    "        body_index = idxToContentMap[body]\n",
    "        tf_head = tfreq[head_index]\n",
    "        tf_body = tfreq[body_index]\n",
    "        tfidf_head = tfidf_vector[head_index]\n",
    "        tfidf_body = tfidf_vector[body_index]\n",
    "        cos = 0.0\n",
    "        if (Headline, body) not in cosineMap:\n",
    "            cos = cosine_similarity( tfidf_head, tfidf_body )[0]\n",
    "            cosineMap[(Headline, body)] = cos\n",
    "        else:\n",
    "            cos = cosineMap[(Headline, body)]\n",
    "        features.append( np.concatenate( [tf_head,tf_body,cos ] ) )\n",
    "\n",
    "    featuresndarray = np.array( features )\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'X': featuresndarray,\n",
    "        'countVectorer': countVectorer\n",
    "    }\n",
    "\n",
    "\n",
    "def encodeLabels(rawStances):\n",
    "# encode the labels\n",
    "    lencoder = LabelEncoder()\n",
    "    lencoder = lencoder.fit(rawStances)\n",
    "    print('Sample class input:','[0,1,2,3]')\n",
    "    y_int = lencoder.transform(rawStances)\n",
    "    print('encoded as',lencoder.inverse_transform([0,1,2,3]))\n",
    "    y_hot_encoded = np_utils.to_categorical( y_int )\n",
    "    y_encoded = y_int\n",
    "    return {\n",
    "        'encoder':lencoder,\n",
    "        'y_hot_encoded':y_hot_encoded,\n",
    "        'y_encoded':y_encoded\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample class input: [0,1,2,3]\n",
      "encoded as ['agree' 'disagree' 'discuss' 'unrelated']\n",
      "Sample class input: [0,1,2,3]\n",
      "encoded as ['agree' 'disagree' 'discuss' 'unrelated']\n"
     ]
    }
   ],
   "source": [
    "processed_train = preprocess(train_data,train_corpus,None,train=True)\n",
    "encodedLabels_train = encodeLabels(train_data['Stance'])\n",
    "train_X = processed_train['X']\n",
    "train_Y = encodedLabels_train['y_encoded']\n",
    "train_Y_hot = encodedLabels_train['y_hot_encoded']\n",
    "\n",
    "countVectorer = processed_train['countVectorer']\n",
    "\n",
    "\n",
    "processed_test = preprocess(test_data,test_corpus,countVectorer,train=False)\n",
    "encodedLabels_test = encodeLabels(test_data['Stance'])\n",
    "test_X = processed_test['X']\n",
    "test_Y = encodedLabels_test['y_encoded']\n",
    "test_Y_hot = encodedLabels_test['y_hot_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ptambvekar/general/43c0bbd943554bb7b3084ef5f201bf2b\n",
      "\n",
      "COMET INFO: old comet version (1.0.37) detected. current: 1.0.38 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ptambvekar/general/9bd6fe046fe34350a1e1465c512db82f\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      " - 2s - loss: 1.1225 - acc: 0.5005\n",
      "Epoch 2/60\n",
      " - 2s - loss: 0.6805 - acc: 0.7156\n",
      "Epoch 3/60\n",
      " - 2s - loss: 0.5301 - acc: 0.7734\n",
      "Epoch 4/60\n",
      " - 2s - loss: 0.4372 - acc: 0.8189\n",
      "Epoch 5/60\n",
      " - 2s - loss: 0.3919 - acc: 0.8370\n",
      "Epoch 6/60\n",
      " - 2s - loss: 0.3404 - acc: 0.8594\n",
      "Epoch 7/60\n",
      " - 2s - loss: 0.2938 - acc: 0.8712\n",
      "Epoch 8/60\n",
      " - 2s - loss: 0.2530 - acc: 0.8890\n",
      "Epoch 9/60\n",
      " - 2s - loss: 0.2621 - acc: 0.8801\n",
      "Epoch 10/60\n",
      " - 2s - loss: 0.2293 - acc: 0.8874\n",
      "Epoch 11/60\n",
      " - 2s - loss: 0.2155 - acc: 0.8914\n",
      "Epoch 12/60\n",
      " - 2s - loss: 0.2018 - acc: 0.8901\n",
      "Epoch 13/60\n",
      " - 2s - loss: 0.1829 - acc: 0.8974\n",
      "Epoch 14/60\n",
      " - 2s - loss: 0.1644 - acc: 0.9094\n",
      "Epoch 15/60\n",
      " - 2s - loss: 0.1715 - acc: 0.9094\n",
      "Epoch 16/60\n",
      " - 2s - loss: 0.1593 - acc: 0.9131\n",
      "Epoch 17/60\n",
      " - 2s - loss: 0.1470 - acc: 0.9295\n",
      "Epoch 18/60\n",
      " - 2s - loss: 0.1423 - acc: 0.9296\n",
      "Epoch 19/60\n",
      " - 2s - loss: 0.1359 - acc: 0.9298\n",
      "Epoch 20/60\n",
      " - 2s - loss: 0.1354 - acc: 0.9356\n",
      "Epoch 21/60\n",
      " - 2s - loss: 0.1190 - acc: 0.9439\n",
      "Epoch 22/60\n",
      " - 2s - loss: 0.1375 - acc: 0.9371\n",
      "Epoch 23/60\n",
      " - 2s - loss: 0.1201 - acc: 0.9427\n",
      "Epoch 24/60\n",
      " - 2s - loss: 0.1142 - acc: 0.9431\n",
      "Epoch 25/60\n",
      " - 2s - loss: 0.1117 - acc: 0.9505\n",
      "Epoch 26/60\n",
      " - 2s - loss: 0.0986 - acc: 0.9546\n",
      "Epoch 27/60\n",
      " - 2s - loss: 0.1080 - acc: 0.9516\n",
      "Epoch 28/60\n",
      " - 2s - loss: 0.1051 - acc: 0.9544\n",
      "Epoch 29/60\n",
      " - 2s - loss: 0.1025 - acc: 0.9533\n",
      "Epoch 30/60\n",
      " - 2s - loss: 0.0945 - acc: 0.9592\n",
      "Epoch 31/60\n",
      " - 2s - loss: 0.1033 - acc: 0.9592\n",
      "Epoch 32/60\n",
      " - 2s - loss: 0.1010 - acc: 0.9571\n",
      "Epoch 33/60\n",
      " - 2s - loss: 0.1176 - acc: 0.9535\n",
      "Epoch 34/60\n",
      " - 2s - loss: 0.0970 - acc: 0.9592\n",
      "Epoch 35/60\n",
      " - 2s - loss: 0.0992 - acc: 0.9597\n",
      "Epoch 36/60\n",
      " - 2s - loss: 0.0878 - acc: 0.9651\n",
      "Epoch 37/60\n",
      " - 2s - loss: 0.0913 - acc: 0.9663\n",
      "Epoch 38/60\n",
      " - 2s - loss: 0.1024 - acc: 0.9638\n",
      "Epoch 39/60\n",
      " - 2s - loss: 0.1095 - acc: 0.9621\n",
      "Epoch 40/60\n",
      " - 2s - loss: 0.0940 - acc: 0.9643\n",
      "Epoch 41/60\n",
      " - 2s - loss: 0.0823 - acc: 0.9674\n",
      "Epoch 42/60\n",
      " - 2s - loss: 0.0838 - acc: 0.9674\n",
      "Epoch 43/60\n",
      " - 2s - loss: 0.0886 - acc: 0.9672\n",
      "Epoch 44/60\n",
      " - 2s - loss: 0.0917 - acc: 0.9674\n",
      "Epoch 45/60\n",
      " - 2s - loss: 0.0902 - acc: 0.9693\n",
      "Epoch 46/60\n",
      " - 2s - loss: 0.0848 - acc: 0.9691\n",
      "Epoch 47/60\n",
      " - 2s - loss: 0.0851 - acc: 0.9708\n",
      "Epoch 48/60\n",
      " - 2s - loss: 0.0916 - acc: 0.9677\n",
      "Epoch 49/60\n",
      " - 2s - loss: 0.0889 - acc: 0.9689\n",
      "Epoch 50/60\n",
      " - 2s - loss: 0.0853 - acc: 0.9677\n",
      "Epoch 51/60\n",
      " - 2s - loss: 0.0833 - acc: 0.9695\n",
      "Epoch 52/60\n",
      " - 2s - loss: 0.0799 - acc: 0.9741\n",
      "Epoch 53/60\n",
      " - 2s - loss: 0.0802 - acc: 0.9736\n",
      "Epoch 54/60\n",
      " - 2s - loss: 0.0770 - acc: 0.9752\n",
      "Epoch 55/60\n",
      " - 2s - loss: 0.0851 - acc: 0.9708\n",
      "Epoch 56/60\n",
      " - 2s - loss: 0.0784 - acc: 0.9751\n",
      "Epoch 57/60\n",
      " - 2s - loss: 0.0781 - acc: 0.9729\n",
      "Epoch 58/60\n",
      " - 2s - loss: 0.0829 - acc: 0.9740\n",
      "Epoch 59/60\n",
      " - 2s - loss: 0.0763 - acc: 0.9737\n",
      "Epoch 60/60\n",
      " - 2s - loss: 0.0823 - acc: 0.9708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cb011e9780>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "c_weights = class_weight.compute_class_weight('balanced', np.unique(train_Y), train_Y)\n",
    "le = encodedLabels_train['encoder']\n",
    "\n",
    "class_weights_dict = dict(zip(le.transform(list(le.classes_)), c_weights))\n",
    "\n",
    "experiment = Experiment(api_key=\"8FN1yPW4wibx67lyHFiRHWzVk\", project_name=\"general\", workspace=\"ptambvekar\")\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=(top_N*2)+1, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "adam = optimizers.Adam(lr=0.001)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.fit(train_X,train_Y_hot,epochs=60,batch_size=32,shuffle=True,verbose=2,class_weight=class_weights_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1968/1968 [==============================] - 1s 519us/step\n",
      "the confusion matrix:\n",
      " [[530  25  17   6]\n",
      " [ 18 140   4   1]\n",
      " [ 35   9 535  32]\n",
      " [  5   3   8 600]]\n",
      "\n",
      "model metrics are: ['loss', 'acc']\n",
      "[0.793403940260168, 0.9171747967479674]\n",
      "\n",
      "The class wise acuracy is (agree,disagree,discuss,unrelated): [0.91695502 0.85889571 0.87561375 0.97402597]\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(test_X, test_Y_hot)\n",
    "pred_Y = model.predict(test_X)\n",
    "\n",
    "cm=confusion_matrix(test_Y_hot.argmax(axis=1),pred_Y.argmax(axis=1))\n",
    "print('the confusion matrix:\\n',cm)\n",
    "print('\\nmodel metrics are:',model.metrics_names)\n",
    "print(loss_and_metrics)\n",
    "\n",
    "class_accu = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('\\nThe class wise acuracy is (agree,disagree,discuss,unrelated):',class_accu.diagonal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('split_model_85accuracy.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try on another dataset:\n",
    "\n",
    "From the given dataset by fakenewschallenge, we picked and gave half of the 'disagree' labeled data to training. We will remove 50% of data of each label, to create a new test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Data #2 :\n",
      "           Headline  Body ID\n",
      "Stance                      \n",
      "agree           952      952\n",
      "disagree        349      349\n",
      "discuss        2232     2232\n",
      "unrelated      9175     9175\n"
     ]
    }
   ],
   "source": [
    "original_test_data = pd.read_csv( 'competition_test_stances.csv' )\n",
    "test_sizes=original_test_data.groupby('Stance').size()\n",
    "for stance in stances:\n",
    "        stance_data = original_test_data.loc[original_test_data['Stance']==str(stance)]\n",
    "        length = int(test_sizes[stance]/2)\n",
    "        stance_data_index=stance_data.head(length).index.tolist()\n",
    "        original_test_data=original_test_data.drop(stance_data_index)\n",
    "print('The Test Data #2 :')\n",
    "print(original_test_data.groupby('Stance').count())\n",
    "original_test_data.to_csv('my_test_stances.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample class input: [0,1,2,3]\n",
      "encoded as ['agree' 'disagree' 'discuss' 'unrelated']\n"
     ]
    }
   ],
   "source": [
    "test_data_two=readFiles('test_bodies.csv','my_test_stances.csv')\n",
    "test_corpus_two=getCorpus(test_data_two['consolidated_data'])\n",
    "processed_test_two = preprocess(test_data_two['consolidated_data'],test_corpus_two,countVectorer,train=False)\n",
    "encodedLabels_test_two = encodeLabels(test_data_two['consolidated_data']['Stance'])\n",
    "test_two_X = processed_test_two['X']\n",
    "test_two_Y = encodedLabels_test_two['y_encoded']\n",
    "test_two_Y_hot = encodedLabels_test_two['y_hot_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12708/12708 [==============================] - 2s 132us/step\n",
      "the confusion matrix:\n",
      " [[ 498  147  233   74]\n",
      " [ 118   92   92   47]\n",
      " [ 602  252 1141  237]\n",
      " [ 374  266  432 8103]]\n",
      "model metrics are:\n",
      " ['loss', 'acc']\n",
      "\n",
      " [2.2398043678596578, 0.7738432483474976]\n",
      "\n",
      "The class wise acuracy is (agree,disagree,discuss,unrelated): [0.52310924 0.26361032 0.51120072 0.88316076]\n"
     ]
    }
   ],
   "source": [
    "#experiment = Experiment(api_key=\"8FN1yPW4wibx67lyHFiRHWzVk\", project_name=\"general\", workspace=\"ptambvekar\")\n",
    "\n",
    "loss_and_metrics_test_two = model.evaluate(test_two_X, test_two_Y_hot)\n",
    "pred_Y_test_two = model.predict(test_two_X)\n",
    "\n",
    "cm_test_two=confusion_matrix(test_two_Y_hot.argmax(axis=1),pred_Y_test_two.argmax(axis=1))\n",
    "print('the confusion matrix:\\n',cm_test_two)\n",
    "print('model metrics are:\\n',model.metrics_names)\n",
    "print('\\n',loss_and_metrics_test_two)\n",
    "\n",
    "test_two_class_accu = cm_test_two.astype('float') / cm_test_two.sum(axis=1)[:, np.newaxis]\n",
    "print('\\nThe class wise acuracy is (agree,disagree,discuss,unrelated):',test_two_class_accu.diagonal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
